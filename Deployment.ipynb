{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cfb11f8-8f59-440b-bbf5-6651c9bb8d73",
   "metadata": {},
   "source": [
    "<h2>Deployment Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aadf071b-c74a-4368-81ce-1110e4467a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\HP\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\HP\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\HP\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2025-03-29 00:37:53.059 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import docx\n",
    "import joblib\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Download NLTK resources only if not already downloaded\n",
    "nltk_data_path = os.path.join(os.path.expanduser(\"~\"), \"nltk_data\")\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "\n",
    "# Initialize text processing tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def extract_text_from_docx(file):\n",
    "    \"\"\"Extract text from DOCX file\"\"\"\n",
    "    doc = docx.Document(file)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs if para.text])\n",
    "\n",
    "def preprocess_resume_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess resume text to match training data format\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters but keep spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize and lemmatize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "@st.cache_resource\n",
    "def load_models():\n",
    "    \"\"\"Load all required models with caching\"\"\"\n",
    "    try:\n",
    "        # Load TF-IDF vectorizer\n",
    "        with open(\"OneDrive/Documents/GitHub/Project_ResumeClassifiaction/tfidf_vectorizer.pkl\", \"rb\") as file:\n",
    "            tfidf = pickle.load(file)\n",
    "        \n",
    "        # Load Gradient Boosting model\n",
    "        with open(\"OneDrive/Documents/GitHub/Project_ResumeClassifiaction/gradient_boosting.pkl\", \"rb\") as file:\n",
    "            model = pickle.load(file)\n",
    "        \n",
    "        # Initialize LabelEncoder (assuming encoder.py contains the class)\n",
    "        with open(\"OneDrive/Documents/GitHub/Project_ResumeClassifiaction/encoder.pkl\", \"rb\") as file:\n",
    "            le = pickle.load(file)\n",
    "        \n",
    "        return tfidf, model, le\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading models: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "def main():\n",
    "    st.title(\"Resume Classification System\")\n",
    "    st.markdown(\"Upload a resume in DOCX format to classify its category\")\n",
    "    \n",
    "    # Load models\n",
    "    tfidf, model, le = load_models()\n",
    "    \n",
    "    if tfidf is None or model is None or le is None:\n",
    "        st.error(\"Failed to load required models. Please check your model files.\")\n",
    "        return\n",
    "    \n",
    "    # File uploader\n",
    "    uploaded_file = st.file_uploader(\"Choose a DOCX resume file\", type=\"docx\")\n",
    "    \n",
    "    if uploaded_file is not None:\n",
    "        # Extract and preprocess text\n",
    "        raw_text = extract_text_from_docx(uploaded_file)\n",
    "        processed_text = preprocess_resume_text(raw_text)\n",
    "        \n",
    "        # Display sections\n",
    "        with st.expander(\"View Extracted Resume Text\"):\n",
    "            st.text(raw_text[:3000] + (\"...\" if len(raw_text) > 3000 else \"\"))\n",
    "        \n",
    "        with st.expander(\"View Processed Text\"):\n",
    "            st.text(processed_text[:2000] + (\"...\" if len(processed_text) > 2000 else \"\"))\n",
    "        \n",
    "        # Transform and predict\n",
    "        text_tfidf = tfidf.transform([processed_text])\n",
    "        prediction = model.predict(text_tfidf)\n",
    "        \n",
    "        # Get probabilities if available\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            probabilities = model.predict_proba(text_tfidf)[0]\n",
    "            top_classes = np.argsort(probabilities)[::-1][:5]  # Top 5 predictions\n",
    "            \n",
    "            st.subheader(\"Classification Results\")\n",
    "            col1, col2 = st.columns(2)\n",
    "            \n",
    "            with col1:\n",
    "                st.metric(\"Predicted Category\", le.inverse_transform(prediction)[0])\n",
    "            \n",
    "            with col2:\n",
    "                st.metric(\"Confidence Score\", f\"{max(probabilities)*100:.1f}%\")\n",
    "            \n",
    "            st.subheader(\"Top 5 Possible Categories\")\n",
    "            for i, class_idx in enumerate(top_classes):\n",
    "                prob = probabilities[class_idx]\n",
    "                st.progress(prob, text=f\"{le.inverse_transform([class_idx])[0]}: {prob*100:.1f}%\")\n",
    "        else:\n",
    "            st.success(f\"Predicted Category: {le.inverse_transform(prediction)[0]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b668bee-fda3-4e84-be61-2615aa1990e6",
   "metadata": {},
   "source": [
    "<h2>Run in Streamlit Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c853afa-b90c-4dc5-9b0d-400d703b5def",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Resume Classification App\n",
    "\n",
    "ðŸ”— Deployed App:**(https://share.streamlit.io/)  \n",
    "ðŸ“‚ GitHub Repository:**(https://github.com/renuka-rathod-18/Project_ResumeClassifiaction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0752ef67-d36e-467a-8ff7-3de925d92f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
